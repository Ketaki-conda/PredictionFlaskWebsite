{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = 'Dataset\\words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_and_gts(partition_split_file):\n",
    "    paths_and_gts = []\n",
    "    with open(partition_split_file) as f:\n",
    "        for line in f:\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            line_split = line.strip().split(' ')\n",
    "            directory_split = line_split[0].split('-')\n",
    "            image_location = f'{data_location}/{directory_split[0]}/{directory_split[0]}-{directory_split[1]}/{line_split[0]}.png'\n",
    "            gt_text = ' '.join(line_split[8:])\n",
    "            paths_and_gts.append([image_location, gt_text])\n",
    "    \n",
    "    return paths_and_gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(img, old_w, old_h, new_w, new_h):\n",
    "    h1, h2 = int((new_h - old_h) / 2), int((new_h - old_h) / 2) + old_h\n",
    "    w1, w2 = int((new_w - old_w) / 2), int((new_w - old_w) / 2) + old_w\n",
    "    img_pad = np.ones([new_h, new_w, 3]) * 255\n",
    "    img_pad[h1:h2, w1:w2, :] = img\n",
    "    return img_pad\n",
    "\n",
    "\n",
    "def fix_size(img, target_w, target_h):\n",
    "    h, w = img.shape[:2]\n",
    "    if w < target_w and h < target_h:\n",
    "        img = add_padding(img, w, h, target_w, target_h)\n",
    "    elif w >= target_w and h < target_h:\n",
    "        new_w = target_w\n",
    "        new_h = int(h * new_w / w)\n",
    "        new_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        img = add_padding(new_img, new_w, new_h, target_w, target_h)\n",
    "    elif w < target_w and h >= target_h:\n",
    "        new_h = target_h\n",
    "        new_w = int(w * new_h / h)\n",
    "        new_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        img = add_padding(new_img, new_w, new_h, target_w, target_h)\n",
    "    else:\n",
    "        \"\"\"w>=target_w and h>=target_h \"\"\"\n",
    "        ratio = max(w / target_w, h / target_h)\n",
    "        new_w = max(min(target_w, int(w / ratio)), 1)\n",
    "        new_h = max(min(target_h, int(h / ratio)), 1)\n",
    "        new_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        img = add_padding(new_img, new_w, new_h, target_w, target_h)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(path, img_w, img_h):\n",
    "    \"\"\" Pre-processing image for predicting \"\"\"\n",
    "    img = cv2.imread(path)\n",
    "    img = fix_size(img, img_w, img_h)\n",
    "\n",
    "    img = np.clip(img, 0, 255)\n",
    "    img = np.uint8(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "    img /= 255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "letters = [' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
    "           '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?',\n",
    "           'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "           'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "           'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "num_classes = len(letters) + 1\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_labels(text):\n",
    "    return list(map(lambda x: letters.index(x), text))\n",
    "\n",
    "def labels_to_text(labels):\n",
    "    return ''.join(list(map(lambda x: letters[int(x)], labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87292, 4316, 4316)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = get_paths_and_gts('Dataset/train_files.txt')\n",
    "valid_files = get_paths_and_gts('Dataset/valid_files.txt')\n",
    "test_files = get_paths_and_gts('Dataset/test_files.txt')\n",
    "len(train_files), len(valid_files), len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4576\n"
     ]
    }
   ],
   "source": [
    "for index, (img_loc, gt_text) in enumerate(train_files):\n",
    "    if 'r06-022-03-05' in img_loc:\n",
    "        print(index)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset\\\\words/r06/r06-022/r06-022-03-05.png', 'more']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files[4576]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dataset\\\\words/g06/g06-042e/g06-042e-05-03.png', 'notice']\n"
     ]
    }
   ],
   "source": [
    "del train_files[4576]\n",
    "print(train_files[4576])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITIZENS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x178dffecfa0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "i = 21\n",
    "print(train_files[i][1])\n",
    "an_img = preprocess(path=train_files[i][0], img_w=128, img_h=64)\n",
    "plt.imshow(an_img.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextImageGenerator:\n",
    "    \n",
    "    def __init__(self, data,\n",
    "                 img_w,\n",
    "                 img_h, \n",
    "                 batch_size, \n",
    "                 i_len,\n",
    "                 max_text_len):\n",
    "        \n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.batch_size = batch_size\n",
    "        self.max_text_len = max_text_len\n",
    "        self.samples = data\n",
    "        self.n = len(self.samples)\n",
    "        self.i_len = i_len\n",
    "        self.indexes = list(range(self.n))\n",
    "        self.cur_index = 0\n",
    "        \n",
    "    def build_data(self):\n",
    "        self.imgs = np.zeros((self.n, self.img_h, self.img_w))\n",
    "        self.texts = []\n",
    "        for i, (img_filepath, text) in enumerate(self.samples):\n",
    "            img = preprocess(img_filepath, self.img_w, self.img_h)\n",
    "            self.imgs[i, :, :] = img\n",
    "            self.texts.append(text)\n",
    "    \n",
    "    def next_sample(self):\n",
    "        self.cur_index += 1\n",
    "        if self.cur_index >= self.n:\n",
    "            self.cur_index = 0\n",
    "            random.shuffle(self.indexes)\n",
    "        return self.imgs[self.indexes[self.cur_index]], self.texts[self.indexes[self.cur_index]]\n",
    "    \n",
    "    def next_batch(self):\n",
    "        while True:\n",
    "            # width and height are backwards from typical Keras convention\n",
    "            # because width is the time dimension when it gets fed into the RNN\n",
    "            X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])\n",
    "            Y_data = np.zeros([self.batch_size, self.max_text_len])\n",
    "            input_length = np.ones((self.batch_size, 1)) * self.i_len\n",
    "            label_length = np.zeros((self.batch_size, 1))\n",
    "                                   \n",
    "            for i in range(self.batch_size):\n",
    "                img, text = self.next_sample()\n",
    "                img = img.T\n",
    "                img = np.expand_dims(img, -1)\n",
    "                X_data[i] = img\n",
    "                Y_data[i, :len(text)] = text_to_labels(text)\n",
    "                label_length[i] = len(text)\n",
    "                \n",
    "            inputs = [X_data, Y_data, input_length, label_length]\n",
    "            outputs = np.zeros([self.batch_size])\n",
    "            yield (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input_length = 30\n",
    "max_text_len = 16\n",
    "img_w = 128\n",
    "img_h = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TextImageGenerator(train_files, img_w, img_h, batch_size, input_length, max_text_len)\n",
    "train_data.build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87291, 64, 128)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = TextImageGenerator(valid_files, img_w, img_h, batch_size, input_length, max_text_len)\n",
    "validation_data.build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as tf_keras_backend\n",
    "\n",
    "tf_keras_backend.set_image_data_format('channels_last')\n",
    "tf_keras_backend.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          [(None, 128, 64, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 64, 64)  640         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 64, 64)  256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 64, 64)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 64, 32, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 64, 32, 128)  73856       max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 32, 128)  512         conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 32, 128)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 32, 16, 128)  0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 32, 16, 256)  295168      max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 16, 256)  1024        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 16, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 32, 16, 256)  590080      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 16, 256)  1024        conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 16, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max3 (MaxPooling2D)             (None, 32, 8, 256)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv2D)                  (None, 32, 8, 512)   1180160     max3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 8, 512)   2048        conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 8, 512)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 32, 8, 512)   2359808     activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 8, 512)   2048        conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 8, 512)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max4 (MaxPooling2D)             (None, 32, 4, 512)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "con7 (Conv2D)                   (None, 32, 4, 512)   1049088     max4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 4, 512)   2048        con7[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 4, 512)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 2048)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32, 64)       131136      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 32, 512)      657408      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 32, 512)      1574912     bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 512)      2048        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32, 80)       41040       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 32, 80)       0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,964,304\n",
      "Trainable params: 7,958,800\n",
      "Non-trainable params: 5,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_data = layers.Input(name='the_input', shape=(128,64,1), dtype='float32')  # (None, 128, 64, 1)\n",
    "\n",
    "# Convolution layer (VGG)\n",
    "iam_layers = layers.Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(input_data)\n",
    "iam_layers = layers.BatchNormalization()(iam_layers)\n",
    "iam_layers = layers.Activation('relu')(iam_layers)\n",
    "iam_layers = layers.MaxPooling2D(pool_size=(2, 2), name='max1')(iam_layers)  # (None,64, 32, 64)\n",
    "\n",
    "iam_layers = layers.Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(iam_layers)\n",
    "iam_layers = layers.BatchNormalization()(iam_layers)\n",
    "iam_layers = layers.Activation('relu')(iam_layers)\n",
    "iam_layers = layers.MaxPooling2D(pool_size=(2, 2), name='max2')(iam_layers)\n",
    "\n",
    "iam_layers = layers.Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(iam_layers)\n",
    "iam_layers = layers.BatchNormalization()(iam_layers)\n",
    "iam_layers = layers.Activation('relu')(iam_layers)\n",
    "iam_layers = layers.Conv2D(256, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(iam_layers)\n",
    "iam_layers = layers.BatchNormalization()(iam_layers)\n",
    "iam_layers = layers.Activation('relu')(iam_layers)\n",
    "iam_layers = layers.MaxPooling2D(pool_size=(1, 2), name='max3')(iam_layers)  # (None, 32, 8, 256)\n",
    "\n",
    "iam_layers = layers.Conv2D(512, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(iam_layers)\n",
    "iam_layers = layers.BatchNormalization()(iam_layers)\n",
    "iam_layers = layers.Activation('relu')(iam_layers)\n",
    "iam_layers = layers.Conv2D(512, (3, 3), padding='same', name='conv6')(iam_layers)\n",
    "iam_layers = layers.BatchNormalization()(iam_layers)\n",
    "iam_layers = layers.Activation('relu')(iam_layers)\n",
    "iam_layers = layers.MaxPooling2D(pool_size=(1, 2), name='max4')(iam_layers)\n",
    "\n",
    "iam_layers = layers.Conv2D(512, (2, 2), padding='same', kernel_initializer='he_normal', name='con7')(iam_layers)\n",
    "iam_layers = layers.BatchNormalization()(iam_layers)\n",
    "iam_layers = layers.Activation('relu')(iam_layers)\n",
    "\n",
    "# CNN to RNN\n",
    "iam_layers = layers.Reshape(target_shape=((32, 2048)), name='reshape')(iam_layers)\n",
    "iam_layers = layers.Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(iam_layers)\n",
    "\n",
    "# RNN layer\n",
    "# layer ten\n",
    "iam_layers = layers.Bidirectional(layers.LSTM(units=256, return_sequences=True))(iam_layers)\n",
    "# layer nine\n",
    "iam_layers = layers.Bidirectional(layers.LSTM(units=256, return_sequences=True))(iam_layers)\n",
    "iam_layers = layers.BatchNormalization()(iam_layers)\n",
    "\n",
    "# transforms RNN output to character activations:\n",
    "iam_layers = layers.Dense(80, kernel_initializer='he_normal', name='dense2')(iam_layers)\n",
    "iam_outputs = layers.Activation('softmax', name='softmax')(iam_layers)\n",
    "\n",
    "labels = layers.Input(name='the_labels', shape=[16], dtype='float32')\n",
    "input_length = layers.Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = layers.Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return tf_keras_backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "\n",
    "# loss function\n",
    "loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([iam_outputs, labels, input_length, label_length])\n",
    "\n",
    "model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class EpochTimeHistory(Callback):\n",
    "    \"\"\"\n",
    "    a custom callback to print the time(in minutes, to console) each epoch took during.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_epoch_times = []\n",
    "        self.valid_epoch_times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        cur_epoch_time = round((time.time() - self.epoch_time_start)/60, 4)\n",
    "        self.train_epoch_times.append(cur_epoch_time)\n",
    "        print(\" ;epoch {0} took {1} minutes.\".format(epoch+1, cur_epoch_time))\n",
    "\n",
    "\n",
    "    def on_test_begin(self, logs={}):\n",
    "        self.test_time_start = time.time()\n",
    "\n",
    "    def on_test_end(self, logs={}):\n",
    "        cur_test_time = round((time.time() - self.test_time_start)/60, 4)\n",
    "        self.valid_epoch_times.append(cur_test_time)\n",
    "        print(\" ;validation took {} minutes.\".format(cur_test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model_save_cb = ModelCheckpoint(filepath='/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch{epoch:02d}-val_loss{val_loss:.3f}.h5',\n",
    "                                verbose=1, save_best_only=False, monitor='val_loss', save_weights_only=False)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=0, mode='min')\n",
    "# reduce_learning_rate_cb = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, cooldown=2, min_lr=0.00001, verbose=1)\n",
    "epoch_times = EpochTimeHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 87291, 4316)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, train_data.n, validation_data.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1364/1363 [==============================] - ETA: -1s - loss: 14.8126 ;validation took 3.3599 minutes.\n",
      "1363/1363 [==============================] - 19320s 14s/step - loss: 14.8085 - val_loss: 5.7076\n",
      "\n",
      "Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data\\two-weights-epoch01-val_loss5.708.h5\n",
      " ;epoch 1 took 322.0466 minutes.\n",
      "Epoch 2/30\n",
      "1364/1363 [==============================] - ETA: 0s - loss: 2.9688  ;validation took 2.8636 minutes.\n",
      "1363/1363 [==============================] - 17014s 12s/step - loss: 2.9685 - val_loss: 1.9568\n",
      "\n",
      "Epoch 00002: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data\\two-weights-epoch02-val_loss1.957.h5\n",
      " ;epoch 2 took 283.6246 minutes.\n",
      "Epoch 3/30\n",
      "1364/1363 [==============================] - ETA: -2s - loss: 1.8049 ;validation took 5.0412 minutes.\n",
      "1363/1363 [==============================] - 46058s 34s/step - loss: 1.8049 - val_loss: 4.8834\n",
      "\n",
      "Epoch 00003: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data\\two-weights-epoch03-val_loss4.883.h5\n",
      " ;epoch 3 took 767.7106 minutes.\n",
      "Epoch 4/30\n",
      " 834/1363 [=================>............] - ETA: 2:04:40 - loss: 1.3700"
     ]
    }
   ],
   "source": [
    "history_model_3 = model.fit(train_data.next_batch(),\n",
    "                            validation_data=validation_data.next_batch(),\n",
    "                            steps_per_epoch=train_data.n/batch_size,\n",
    "                            validation_steps=validation_data.n // batch_size,\n",
    "                            epochs=30,\n",
    "                            callbacks=[earlystop, model_save_cb, epoch_times])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filepath='handwritten-model.h5', overwrite=False, include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
